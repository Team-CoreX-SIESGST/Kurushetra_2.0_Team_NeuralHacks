"""
Embeddings service for OmniSearch AI.
Handles text embedding generation using sentence transformers.
"""

import numpy as np
from typing import List, Union
from sentence_transformers import SentenceTransformer
from app.settings import settings

class EmbeddingsService:
    """Service for generating and managing text embeddings."""
    
    def __init__(self):
        self.model_name = "all-MiniLM-L6-v2"  # Lightweight, fast model
        self.model = None
        self.embedding_dim = 384
        
    async def initialize(self):
        """Initialize the embedding model."""
        try:
            self.model = SentenceTransformer(self.model_name)
            print(f"Embeddings model {self.model_name} loaded successfully")
        except Exception as e:
            print(f"Failed to load embeddings model: {e}")
            # Fallback to a simpler approach if needed
            self.model = None
    
    def generate_embeddings(self, texts: Union[str, List[str]]) -> np.ndarray:
        """Generate embeddings for given texts."""
        if not self.model:
            raise RuntimeError("Embeddings model not initialized")
        
        if isinstance(texts, str):
            texts = [texts]
        
        try:
            embeddings = self.model.encode(texts, convert_to_numpy=True)
            return embeddings
        except Exception as e:
            print(f"Embedding generation failed: {e}")
            # Return zero embeddings as fallback
            return np.zeros((len(texts), self.embedding_dim))
    
    def generate_single_embedding(self, text: str) -> np.ndarray:
        """Generate embedding for a single text."""
        return self.generate_embeddings([text])[0]
    
    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Compute cosine similarity between two embeddings."""
        try:
            # Normalize embeddings
            norm1 = np.linalg.norm(embedding1)
            norm2 = np.linalg.norm(embedding2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            # Compute cosine similarity
            similarity = np.dot(embedding1, embedding2) / (norm1 * norm2)
            return float(similarity)
        except Exception as e:
            print(f"Similarity computation failed: {e}")
            return 0.0
    
    def batch_similarity(self, query_embedding: np.ndarray, candidate_embeddings: np.ndarray) -> np.ndarray:
        """Compute similarities between query and multiple candidates."""
        try:
            # Normalize embeddings
            query_norm = np.linalg.norm(query_embedding)
            candidate_norms = np.linalg.norm(candidate_embeddings, axis=1)
            
            # Avoid division by zero
            valid_indices = candidate_norms > 0
            
            similarities = np.zeros(len(candidate_embeddings))
            if query_norm > 0:
                similarities[valid_indices] = np.dot(
                    candidate_embeddings[valid_indices], 
                    query_embedding
                ) / (candidate_norms[valid_indices] * query_norm)
            
            return similarities
        except Exception as e:
            print(f"Batch similarity computation failed: {e}")
            return np.zeros(len(candidate_embeddings))
    
    def get_embedding_dimension(self) -> int:
        """Get the dimension of embeddings generated by this model."""
        return self.embedding_dim
